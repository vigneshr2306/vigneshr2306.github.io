<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Vignesh  Ravikumar | FCOS Fully Convolutional One-Stage Object Detection</title>
    <meta name="author" content="Vignesh  Ravikumar" />
    <meta name="description" content="FCOS Fully Convolutional One-Stage Object Detection Paper Review" />
    <meta name="keywords" content="robotics, 3D computer vision, deep learning" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://vigneshr2306.github.io/blog/2022/FCOS/">
    
    <!-- Dark Mode -->
    

    <!-- Google Analytics -->
    

  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-224232394-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-224232394-1');
</script>
  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://vigneshr2306.github.io/"><span class="font-weight-bold" style="color: #03aaff">Vignesh</span>   Ravikumar</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/assets/pdf/Vignesh_Ravikumar_Resume.pdf" target="_blank">resume</a>
              </li>
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/assets/pdf/Sundar_CV.pdf" target="_blank">vitae</a>
              </li> -->

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">FCOS Fully Convolutional One-Stage Object Detection</h1>
    <p class="post-meta">October 27, 2022</p>
    <p class="post-tags">
      <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>
        ·  
        <a href="/blog/category/paper-review">
          <i class="fas fa-tag fa-sm"></i> paper-review</a>  
          

    </p>
  </header>

  <article class="post-content">
    <p>Citation: Z. Tian, C. Shen, H. Chen and T. He, “FCOS: Fully Convolutional One-Stage Object Detection,” 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 9626-9635, doi: 10.1109/ICCV.2019.00972.<br><br>
Brief Summary:<br>
Object Detection has been growing at a rapid pace with various applications in autonomous vehicles, mobile robots, etc. State-of-the-art detectors at that time, like Faster R-CNN use anchor boxes of three specific aspect ratios and sizes for detecting multiple objects in an image. However, this is still a hyperparameter that needs to be tuned carefully to get higher performance. This paper overcomes this issue by proposing an anchorless method of object detection known as Fully Convolutional One-Stage (FCOS) Object Detection. By using ResNeXt-64x4d-101, the model achieves 44.7% in Average Precision (AP) with single-model and single-scale testing, surpassing previous one-stage detectors. On top of this, they additionally propose a “center-ness” approach which helps suppress the low-quality detected bounding boxes and improves overall performance by a large margin. Since the approach is a per-pixel prediction method, it helps in reusing the semantic segmentation pipeline in deployment.<br><br></p>

<p>Main Contributions:<br>
The paper proposes an anchor-free method of object detection which helps in reducing the computations by a huge margin.
A novel approach called “centerness” is presented, which helps in suppressing the low-quality detected bounding boxes.
The method proposed helps in generalizing the concept of object detection with other FCN-solvable tasks such as semantic segmentation.
Strengths:
The paper achieves state-of-the-art results among one-stage object detectors.
Avoiding the anchor box helps in avoiding complex computations such as Intersection-Over-Union (IOU).
FCOS can be replaced with Regional Proposal Networks (RPN) in two-stage detectors that enhance the performance of anchor-based methods.
This paper can be extended to tasks such as instance segmentation and key-point detection.
The paper is well-written; the code is provided for reproducibility and has enough mathematical explanations and supporting figures.<br><br></p>

<p>Weakness:<br>
There are assumptions for many parts of the paper without sufficient proof, which can be confusing to the readers.<br><br></p>

<p>In-Depth Analysis of Strengths and Weaknesses:<br>
The paper with its anchor-free detection method scores over the anchor-based methods such as Faster R-CNN by avoiding complicated IOU computation resulting in faster training times. The novel “center-ness” approach also helped in increasing training time, as it helped in reducing the number of outliers that the Non-Maximum Suppression has to deal with. Avoiding the anchor-box method also helped in overcoming complex computations such as Intersection-Over-Union. All these helped in enhancing the efficiency of the model by reducing computation time, which led to the state-of-the-art status of the paper.
FCOS can also be used along with anchor-based methods such as Faster R-CNN by using it for the Regional Proposal Networks to improve the performance of those models.
It is not described how several levels in multi-level prediction with FPN affect its mean Average Precision.<br><br>
Experimental Results:<br>
The experiments were conducted on the MS-COCO dataset with ResNet-50 as the backbone network. The ablation study is done in an extensive manner, carefully analyzing the effectiveness of the proposed architecture.<br>
Multi-Level Prediction with FPN: To avoid overlapping bounding boxes, the problems of poor BPR and ambiguous samples must be fixed. They obtained a BPR of roughly 95.55%, which is greater than anchor-based detectors. They get an even greater BPR of 98.40% with FPN included. They outperform anchor-based models with their FCOS because they obtain a substantially smaller proportion of unclear samples.
Center-ness enhances the bounding box quality by adding center-ness, and they found that this raised AP performance to 37.1%, exceeding RetinaNet’s 35.9%. They also eliminated the requirement for computing IOU.
FCOS achieves 43.2% in AP using ResNeXt-64x4d-101-FPN as their guiding principle. It performs far better than the SOTA anchor-free detector CornerNet while requiring substantially less processing.<br><br>
Extensions:<br>
The FCOS-based object detection can be extended to areas such as semantic segmentation, instance segmentation, and keypoint detection, as the per-pixel prediction methods are analogous to it.<br>
Comments:<br>
The ablation study for every component of the architecture helps to analyze and understand the paper in a better way.</p>

  </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Vignesh  Ravikumar. 
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-224232394-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-224232394-1');
  </script>
  </body>
</html>

