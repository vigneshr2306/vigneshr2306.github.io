<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="https://vigneshr2306.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://vigneshr2306.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-01-14T21:52:42+00:00</updated><id>https://vigneshr2306.github.io/feed.xml</id><title type="html">blank</title><subtitle>My personal webpage. 
</subtitle><entry><title type="html">MViTv2 Improved multiscale vision transformers for classification and detection</title><link href="https://vigneshr2306.github.io/blog/2022/MVitv2/" rel="alternate" type="text/html" title="MViTv2 Improved multiscale vision transformers for classification and detection" /><published>2022-11-04T23:13:27+00:00</published><updated>2022-11-04T23:13:27+00:00</updated><id>https://vigneshr2306.github.io/blog/2022/MVitv2</id><content type="html" xml:base="https://vigneshr2306.github.io/blog/2022/MVitv2/"><![CDATA[<p>Citation: Li, Yanghao and Wu, Chao-Yuan and Fan, Haoqi and Mangalam, Karttikeya and Xiong, Bo and Malik, Jitendra and Feichtenhofer, Christoph, “MViTv2: Improved multiscale vision transformers for classification and detection” 2021 IEEE Conference on Computer Vision and Pattern Recognition.<br /><br />
Brief Summary:<br />
Convolutional neural networks (CNNs) are beginning to be replaced by Vision Transformers (ViTs) in many computer vision tasks. However, CNNs continue to possess architectural advantages over ViTs in computer vision tasks. For instance, locality is a key component of the convolution technique that CNNs inherit when handling image input (since images tend to hold locally-similar properties). The pooling of layers inside the network can give computational efficiency and hierarchical higher-level feature learning at the top layers, which is another aspect of CNN architecture. The paper on MViT introduced pooling layers to improve ViTs and this paper is an extension of that work. For improved accuracy and computational efficiency, this work provides decomposed relative positional embeddings and residual pooling connections. Additionally, it attained SOTA in the three Visual Recognition categories that received the highest evaluations. MViTv2 boasts cutting-edge performance: 86.1% on Kinetics-400 video classification; 88.8% accuracy on ImageNet classification, and 56.1 APbox on COCO object identification.<br /><br /></p>

<p>Main Contributions:<br />
Leveraging decomposed location distances to insert position information into Transformer blocks through shift-invariant positional embeddings.
A residual pooling connection to offset the impact of pooling strides on the computation of attention.
Combining hybrid window attention with attention pooling helps improve the accuracy/computing trade-off.
Strengths:
This paper achieves state-of-the-art results in ImageNet classification with 88.8% accuracy.
This paper shows better performance than other ViT architectures such as Swin Transformer.
Not just classification, they have also explored domains such as Object Detection and Video Recognition with state-of-the-art results.
The paper is well-written; the code is provided for reproducibility, and has enough mathematical explanations and supporting figures.<br /><br />
Weakness:<br />
When compared to MViTv1, this paper introduces very few changes to the existing architecture, which is primitive.
The paper does not discuss where the model would fail.<br /><br /></p>

<p>In-Depth Analysis of Strengths and Weaknesses:<br />
The MViT paper relies heavily on absolute position embedding of the patches of images, which ignores the fundamental principle of shift-invariance. This paper adds a relative positional embedding concept that only depends on the relative location distance between tokens into the pooled self-attention computation. Along the spatiotemporal axes, distant computation is decomposed to reduce complexity. <br /><br />
To increase information flow and facilitate the training of pooling attention blocks in MViT, residual pooling connections are used. Due to the cheap cost of adding the pooled query sequence, this update continues to benefit from low-complexity attention computation with significant advancements in key and value pooling.<br /><br />
Self-Attention used in transformers is computationally expensive based on the number of tokens used. The Hybrid type Pooling Attention along with Feature Pyramid Network is used in this work for each window, and it outperformed the Swin Transformer in terms of performance.<br /><br />
The paper achieves state-of-the-art results in ImageNet Classification, Object Detection, and Video Recognition. The experimental results are explained in the next section. The paper is explained very well, with supporting mathematical explanations and equations. However, the changes made in the paper seem primitive to the previous iteration of MViT and do not significantly improve the results. The paper also does not discuss the possible limitations.<br /><br /></p>

<p>Experimental Results:<br />
In this study, multiscale vision transformers were used to test three downstream tasks.
Classification: The state-of-the-art on the ImageNet leaderboard, MViTv2 achieves roughly 1% greater accuracy with 10% fewer flops and parameters compared to MViTv1 and considerable gains over Swin and DeiT transformers. Additionally, SOTA’s performance in the community’s most recent full-size crop testing is reported by MViTv2.
Object Detection: In terms of object detection, the MaskRCNN framework’s MViTv2 backbone outperformed transformer-based backbones (Swin, ViL, and MViTv1) as well as CNN backbones (ResNet, ResNext, etc.). MViTv2 displays a 2.5 AP box/AP mask improvement over Swin transformer backbone. Similar encouraging patterns were observed in tests using the Cascade MaskRCNN architecture.
Self-attention ablation studies: In COCO and ImageNet (IN) assessments, the MViTv2 attention mechanisms (pooling and Hwin) are contrasted with alternative attention mechanisms (global(full), windowed, and Shiftedwindow). The following findings were made:
When there are no cross-window connections in the default window of ViTB-based models, accuracy in the top-1 reduces by 2% but FLOPs and memory consumption go down. Hwin window outperforms Swin by +1.7% while Swin attention only recovers +0.4%. With 38% fewer failures, attention pooling exhibits the best accuracy/computational trade-offs. When Hwin and attention are combined, object detection performance is at its highest.<br /><br />
Extensions:<br />
This paper can be extended to various vision applications such as instance segmentation as a vision backbone. It can also be used in applications such as visual inspection and quality management in manufacturing, cancer and tumor detection in healthcare, and vehicle re-identification and pedestrian detection in transportation.<br /><br />
Comments:<br />
MViT, which introduced general hierarchical architecture structure to visual recognition, showed better performance than other ViT architectures (e.g. Swin Transformer) and achieved SOTA in various tasks such as classification, object detection, instance segmentation, and video recognition. Transformer-based architecture is expected to be widely used in various studies in the future as it is rapidly developing.</p>]]></content><author><name></name></author><category term="paper-review" /><summary type="html"><![CDATA[MViTv2 Improved multiscale vision transformers for classification and detection Paper Review]]></summary></entry><entry><title type="html">Masked Autoencoders Are Scalable Vision Learners</title><link href="https://vigneshr2306.github.io/blog/2022/MAE/" rel="alternate" type="text/html" title="Masked Autoencoders Are Scalable Vision Learners" /><published>2022-11-04T23:13:27+00:00</published><updated>2022-11-04T23:13:27+00:00</updated><id>https://vigneshr2306.github.io/blog/2022/MAE</id><content type="html" xml:base="https://vigneshr2306.github.io/blog/2022/MAE/"><![CDATA[<p>Citation: He, Kaiming et al. “Masked Autoencoders Are Scalable Vision Learners.” 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022): 15979-15988. <br /><br />
Brief Summary:<br />
An autoencoder is used to learn efficient coding of unlabeled data. By making an attempt to regenerate the input from the encoding, the encoding is validated and improved. By teaching the network to disregard irrelevant data (or “noise”), the autoencoder learns a representation (encoding) for a set of data, generally for dimensionality reduction. This paper introduces Masked Autoencoders where random patches of the input images are masked and the rebuilding is done on the missing pixels. On a standard ViT-Huge model calibrated on ImageNet-1K, they use this scalable self-supervised architecture and obtain 87.8% accuracy with a 3X shorter pre-training time, making MAE scalable for huge models.<br /><br /></p>

<p>Main Contributions:<br />
An asymmetric architecture that enables the encoder to work solely on the partial, seen signal (without mask tokens) and use a lightweight decoder that reconstructs the entire signal from the latent representation and the mask tokens.
Masking the majority of inputs generates a nontrivial and meaningful self-supervisory task.<br /><br />
Strengths:<br />
This paper leverages on Vision Transformer’s ability to bridge the architectural gap that CNNs were possessing - ability to integrate mask tokens which were not possible on CNNs.
The concept is very scalable and simple to deploy.
The paper is well-written; the code is provided for reproducibility, and has enough mathematical explanations and supporting figures.<br /><br />
Weakness:<br />
There is no mathematical formulation or justification for their method.</p>

<p>In-Depth Analysis of Strengths and Weaknesses:<br />
The suggested MAE in this study is a straightforward autoencoder that makes full use of partial observation (the input picture is incomplete) to produce a complete image. With the exception of its unique asymmetric architecture, this autoencoder is nearly identical to other earlier (classical) autoencoders. The model can avoid training on every pixel in the image thanks to this approach.
Masking the majority of inputs generates a nontrivial and meaningful self-supervisory task. The success of both designs, as previously indicated, demonstrates our ability to train our models with big train datasets successfully and efficiently. It indicates that the accuracy has increased and that acceleration training has sped up by at least three times. This scalable approach makes it viable for learning high-capacity models that generalize well (this can be seen in experiment results).
However, the paper does not provide enough mathematical formulations or justifications for their method.<br /><br />
Experimental Results:<br />
Ablation studies were performed on several parts of the architecture such as masking ratio, decoder design, mask token, reconstruction target, data augmentation, mask sampling strategy, and training schedule. <br />
Comparisons with previous results:<br /><br />
Comparisons with self-supervised methods: This study reveals that MAE is readily scaled up and exhibits consistent progress with larger models. ViT-H provides 86.9% accuracy for MAE (224 size). A 448 size is used for fine-tuning, which results in 87.8% accuracy. BEiT performs worse than MAE despite being faster and simpler to use.
Comparisons with supervised pre-training: When trained in IN1K, ViT-L degrades, but MAE performs better, and accuracy reaches saturation. MAE performs better with models of greater capacity.<br />
Partial Fine-tuning: Results from probing and fine-tuning are mostly uncorrelated. Only one Transformer block has to be fine-tuned in order to considerably increase accuracy from 73.5% to 81.0%. When compared to MAE, MoCo v3 has a greater linear probing accuracy, but all of its partial fine-tuning outcomes are inferior to those of MAE. When four blocks are tuned, the gap is 2.6%.<br />
Transfer Learning Experiments:
Object detection and segmentation: MAE outperforms supervised, MoCo v3 and BEiT with an APbox of 50.3 for ViT-B and 53.3 for ViT-L. The token-based BEiT is inferior to or on par with the pixel-based MAE, which is also more quicker and easier to use.<br />
Semantic segmentation: Results from pre-training are substantially better than from supervised pre training. The MAE based on pixels surpasses the BEiT based on tokens.<br />
Classification task: The study technique exhibits strong scaling behavior on iNat. With larger models, accuracy increases noticeably and outperforms the prior best results by a wide margin.<br />
Pixels vs. tokens: A study demonstrates that tokenization is not required for MAE since the use of dVAE tokens, while superior to unnormalized pixels in certain circumstances, is statistically equivalent to using normalized pixels in all cases.<br /><br />
Extensions:<br />
Dimensionality reduction, image compression, image denoising, feature extraction, sequence-to-sequence prediction, and recommendation systems are other applications for MAE.<br /><br />
Comments:<br />
In real-world computer vision tasks, this initiative technique is advantageous and useful, particularly for difficult problems with complexity. The model uses less energy and requires fewer inputs for pre-training by removing random patches.</p>]]></content><author><name></name></author><category term="paper-review" /><summary type="html"><![CDATA[Masked Autoencoders Are Scalable Vision Learners Paper Review]]></summary></entry><entry><title type="html">FCOS Fully Convolutional One-Stage Object Detection</title><link href="https://vigneshr2306.github.io/blog/2022/FCOS/" rel="alternate" type="text/html" title="FCOS Fully Convolutional One-Stage Object Detection" /><published>2022-10-27T23:13:27+00:00</published><updated>2022-10-27T23:13:27+00:00</updated><id>https://vigneshr2306.github.io/blog/2022/FCOS</id><content type="html" xml:base="https://vigneshr2306.github.io/blog/2022/FCOS/"><![CDATA[<p>Citation: Z. Tian, C. Shen, H. Chen and T. He, “FCOS: Fully Convolutional One-Stage Object Detection,” 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 9626-9635, doi: 10.1109/ICCV.2019.00972.<br /><br />
Brief Summary:<br />
Object Detection has been growing at a rapid pace with various applications in autonomous vehicles, mobile robots, etc. State-of-the-art detectors at that time, like Faster R-CNN use anchor boxes of three specific aspect ratios and sizes for detecting multiple objects in an image. However, this is still a hyperparameter that needs to be tuned carefully to get higher performance. This paper overcomes this issue by proposing an anchorless method of object detection known as Fully Convolutional One-Stage (FCOS) Object Detection. By using ResNeXt-64x4d-101, the model achieves 44.7% in Average Precision (AP) with single-model and single-scale testing, surpassing previous one-stage detectors. On top of this, they additionally propose a “center-ness” approach which helps suppress the low-quality detected bounding boxes and improves overall performance by a large margin. Since the approach is a per-pixel prediction method, it helps in reusing the semantic segmentation pipeline in deployment.<br /><br /></p>

<p>Main Contributions:<br />
The paper proposes an anchor-free method of object detection which helps in reducing the computations by a huge margin.
A novel approach called “centerness” is presented, which helps in suppressing the low-quality detected bounding boxes.
The method proposed helps in generalizing the concept of object detection with other FCN-solvable tasks such as semantic segmentation.
Strengths:
The paper achieves state-of-the-art results among one-stage object detectors.
Avoiding the anchor box helps in avoiding complex computations such as Intersection-Over-Union (IOU).
FCOS can be replaced with Regional Proposal Networks (RPN) in two-stage detectors that enhance the performance of anchor-based methods.
This paper can be extended to tasks such as instance segmentation and key-point detection.
The paper is well-written; the code is provided for reproducibility and has enough mathematical explanations and supporting figures.<br /><br /></p>

<p>Weakness:<br />
There are assumptions for many parts of the paper without sufficient proof, which can be confusing to the readers.<br /><br /></p>

<p>In-Depth Analysis of Strengths and Weaknesses:<br />
The paper with its anchor-free detection method scores over the anchor-based methods such as Faster R-CNN by avoiding complicated IOU computation resulting in faster training times. The novel “center-ness” approach also helped in increasing training time, as it helped in reducing the number of outliers that the Non-Maximum Suppression has to deal with. Avoiding the anchor-box method also helped in overcoming complex computations such as Intersection-Over-Union. All these helped in enhancing the efficiency of the model by reducing computation time, which led to the state-of-the-art status of the paper.
FCOS can also be used along with anchor-based methods such as Faster R-CNN by using it for the Regional Proposal Networks to improve the performance of those models.
It is not described how several levels in multi-level prediction with FPN affect its mean Average Precision.<br /><br />
Experimental Results:<br />
The experiments were conducted on the MS-COCO dataset with ResNet-50 as the backbone network. The ablation study is done in an extensive manner, carefully analyzing the effectiveness of the proposed architecture.<br />
Multi-Level Prediction with FPN: To avoid overlapping bounding boxes, the problems of poor BPR and ambiguous samples must be fixed. They obtained a BPR of roughly 95.55%, which is greater than anchor-based detectors. They get an even greater BPR of 98.40% with FPN included. They outperform anchor-based models with their FCOS because they obtain a substantially smaller proportion of unclear samples.
Center-ness enhances the bounding box quality by adding center-ness, and they found that this raised AP performance to 37.1%, exceeding RetinaNet’s 35.9%. They also eliminated the requirement for computing IOU.
FCOS achieves 43.2% in AP using ResNeXt-64x4d-101-FPN as their guiding principle. It performs far better than the SOTA anchor-free detector CornerNet while requiring substantially less processing.<br /><br />
Extensions:<br />
The FCOS-based object detection can be extended to areas such as semantic segmentation, instance segmentation, and keypoint detection, as the per-pixel prediction methods are analogous to it.<br />
Comments:<br />
The ablation study for every component of the architecture helps to analyze and understand the paper in a better way.</p>]]></content><author><name></name></author><category term="paper-review" /><summary type="html"><![CDATA[FCOS Fully Convolutional One-Stage Object Detection Paper Review]]></summary></entry><entry><title type="html">Recurrent Neural Networks With Intra-Frame Iterations for Video Deblurring</title><link href="https://vigneshr2306.github.io/blog/2022/RNN/" rel="alternate" type="text/html" title="Recurrent Neural Networks With Intra-Frame Iterations for Video Deblurring" /><published>2022-10-13T23:13:27+00:00</published><updated>2022-10-13T23:13:27+00:00</updated><id>https://vigneshr2306.github.io/blog/2022/RNN</id><content type="html" xml:base="https://vigneshr2306.github.io/blog/2022/RNN/"><![CDATA[<p>Citation: S. Nah, S. Son, and K. M. Lee, “Recurrent Neural Networks With Intra-Frame Iterations for Video Deblurring,” 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 8094-8103, doi: 10.1109/CVPR.2019.00829. <br /><br />
Brief Summary:<br />
This paper suggests a novel video deblurring method that beats the state-of-the-art performance using Recurrent Neural Networks (RNN). This real-time deblurring method exploits the temporal relationship between the past frame and the current frame to improve the accuracy of the model. This method does not use any additional modules, which helps in updating the weights of the iterative model in a stochastic manner. Experimental results on the GOPRO dataset show the Peak Signal-to-Noise Ratio to be 29.97.<br /><br /></p>

<p>Main Contributions:<br />
The model produces better results without modifying the architecture by updating the hidden states multiple times internally during a single time step.<br />
More optimal hidden state update strategies such as partial recurrence and different types of intra-frame iteration strategies are explained clearly.<br />
The data term in the loss function minimizes the restoration error and a prior term that favors a shorter computation path.<br />
The deblurring accuracy and the computational efficiency beats the state-of-the-art models. This can be inferred from the experimental results in GOPRO dataset.<br /><br />
Strengths:<br />
Exploits both inter-frame and intra-frame recurrent schemes which are simple yet effective.<br />
The previous literature is explained comprehensively, which gives good background knowledge about deblurring to the reader.<br />
Strong experimental results and algorithms help in understanding the concept clearly.<br />
The figures and tables used in the paper convey the technical work intuitively.<br /><br />
Weakness:<br />
The paper does not contain code resources for reproducing the results.
The captioning of the figures is not clear and the paper contains many grammatical errors.
The paper heavily relies on abbreviations which might confuse the readers.<br /><br /></p>

<p>In-Depth Analysis of Strengths and Weaknesses:<br />
The main strength of the paper is to exploit the temporal relationship between neighboring frames to improve the accuracy of the deblurring model. The single-cell and dual-cell approaches differ in which parameters are used for updating the hidden states. The former uses the same parameters to estimate both the initial hidden state and the updated hidden state, while the latter uses two RNN cells where the second cell is used to update the hidden states and predict latent frames. <br />
The previous literature on video deblurring, burst deblurring, and stochastic neural networks was comprehensively explained. The experimental results on the GOPRO dataset and Deep video deblurring for the hand-held camera dataset were solid, which is explained in the next section.<br />
Even though the paper has several strengths, it has some major flaws. The inability to reproduce the results without coding resources is difficult for researchers to further improve on the same line of research. Additionally, the presence of grammatical errors and confusing abbreviations make the paper less readable.<br /><br /></p>

<p>Experimental Results:<br />
The experiments were conducted on the GOPRO dataset with 2103 training samples from 22 sequences, and 1111 evaluation samples from 11 sequences. From this dataset, blur and sharp image pairs were generated. The original video resolution was downsampled from 1280 × 720 to 960 × 540 before averaging to suppress the noise and video compression artifacts. All the models are implemented using PyTorch 0.4.1, which was created with CUDA 9.2 and cuDNN 7.1, on NVIDIA GTX 1080 Ti GPUs. Another dataset consisting of 61 sequences containing 5708 training pairs and 10 sequences including 1000 evaluation pairs was synthesized from 240fps videos. PSNR and SSIM were used to evaluate the models. <br />
Comparisons on GOPRO Dataset: <br />
In the GOPRO dataset, they achieved a deblurring accuracy of 29.97 PSNR and 0.8947 SSIM. This result is better than the existing state-of-the-art, and this improved result is due to the proposed intra-frame iteration scheme and the stochastic training methods.<br />
Comparisons on Deep video deblurring for hand-held cameras Dataset and Real Videos:<br />
For this dataset, the deblurring accuracy was 30.80 PSNR and 0.8991 SSIM. The result on real videos clarifies blurred textures.<br /><br />
Extensions:<br />
The architecture of this model can be further improved by the latest architectures such as Long Short Term Memory (LSTM) and transformers. <br /><br />
Comments:<br />
This paper paves way for research in image deblurring that can be used in applications such as medical imaging, microscopy, remote sensing, and planetary imaging.</p>]]></content><author><name></name></author><category term="paper-review" /><summary type="html"><![CDATA[Recurrent Neural Networks With Intra-Frame Iterations for Video Deblurring Paper Review]]></summary></entry><entry><title type="html">Group Normalization</title><link href="https://vigneshr2306.github.io/blog/2022/Group_Normalization/" rel="alternate" type="text/html" title="Group Normalization" /><published>2022-10-07T23:13:27+00:00</published><updated>2022-10-07T23:13:27+00:00</updated><id>https://vigneshr2306.github.io/blog/2022/Group_Normalization</id><content type="html" xml:base="https://vigneshr2306.github.io/blog/2022/Group_Normalization/"><![CDATA[<p>Citation: Wu, Yuxin &amp; He, Kaiming. (2018). Group Normalization, European Conference on Computer Vision.<br /><br />
Brief Summary:<br />
This paper proposes a new type of normalization technique for deep neural networks called Group Normalization (GN), which involves computing the mean and variance for normalization based on dividing the channels into groups. This technique is advantageous compared to Batch Normalization (BN), where the same parameters were calculated based on dividing the samples into batches. The latter technique was not able to perform well with smaller batch sizes, which is where GN scores as it is not sensitive to batch sizes. GN performs better than BN in several tasks like segmentation, and detection, and on the ImageNet dataset trained with ResNet-50 architecture with a batch size of 2, GN has a 10.6% lower error rate compared to BN. <br /><br />
Main Contributions:<br />
The idea of considering deep neural network features as structured vectors that can be grouped together.
A novel normalization technique that beats the state-of-the-art normalization techniques with its lesser error rate.
Independent to batch size helps in drastically reducing the error rates.
Increases the pool of domains where normalization can be performed because of its adaptability to huge model-size applications.<br /><br />
Strengths:<br />
Stable results were shown using Group Normalization with different batch sizes and datasets.
Group Normalization can naturally transfer from pretraining to fine-tuning.
The paper is well-written, with good structure, and is backed by several experimental results and figures.<br /><br />
Weakness:<br />
The only weakness of this research work is that all the previous state-of-the-art models are tuned for BN, which makes GN less popular as it is difficult for tuning hyperparameters.<br /><br />
In-Depth Analysis of Strengths and Weaknesses:<br />
The key idea behind Group Normalization is to not think of deep neural network features as unstructured vectors, but consider the corresponding channels of these features to be normalized together. The coefficients of these vectors can be interdependent; each group of channels is assumed to have a shared mean and variance, which leads to GN.<br />
Since the grouping is done across the channels, GN is independent of batch size and can work relatively well with batch sizes as small as 2, to higher batch sizes as well. Due to this property, GNs are independent of batch size, which acts as a huge advantage compared to BN.<br />
This independence to batch size opens a plethora of opportunities for GN to be implemented. There were heavy trade-offs between model size and batches in applications such as object detection, and segmentation, which was not the case with GNs. Hence, it was able to produce 10.6% lower error rates compared to BN in the ImageNet dataset.
GNs have the ability to naturally change to fine-tuning from pre-training, even though they use different batch sizes. The results of GNs in this paper were stable and comprehensive, tested with different datasets and batch sizes, which will be explained in the next section.<br />
Overall, the quality of the paper is good, with clear notations and enough experiments to support their hypothesis. The previous literature was clearly explained with intuitive figures which makes it easy to understand the paper.<br />
Even though GN outperforms BN almost everywhere, BN is still the most popular choice in the industry for normalization, due to several state-of-the-art models and their hyperparameters tuned for BN, and the results of papers not being able to do fair comparisons otherwise.<br /><br />
Experimental Results:<br />
In the Image Classification experiment in the ImageNet dataset with ResNet architecture, BN, GN, Layer Norm. (LN) and Instance Norm. (IN) were compared for batch sizes 2 to 32 with increments in powers of 2. It can be seen that for higher batch sizes like 32, the error rate of BN is better than GN and GN approaches BN with degradation of 0.5%. As batch size decreases, the error rate of GN improves significantly and achieves a 10.6% lower error rate than BN. Batch Renorm. is also compared with GN, but GN outperforms it by 2.1%. Experiments were also conducted with different number of groups and different channels per group and the results were compared with the ideal channel number which has the lowest error rate.<br /><br />
Using high-resolution images for Object Detection and Segmentation in the COCO dataset results in a lesser batch size; hence BN layer is linearized to a pre-computed frozen model called BN<em>. Mask R-CNN is used as a baseline, conv4 is used as a backbone, and the Average Precision (AP) is reported. It is seen that GN improves over BN</em> by 1.1 box AP and 0.8 mask AP. It is also trained on the FPN backbone, where the final ResNet-50 GN model is 2.2 points box AP and 1.6 points mask AP better than its BN* variant. Training in the Mask R-CNN from scratch, GN was able to achieve 41.0 box AP and 36.4 mask AP, which are the best results for the COCO dataset from scratch.
In the Video Classification with Kinetics dataset, Inflated 3D (I3D) with ResNet-50 I3D baseline was used. Extending the normalization over the temporal axis, we train in the 400-class set with 32 and 64-frame input clips. For the batch size of 8 with 32 frame input, BN performs better than GN by 0.3% top-1 accuracy and 0.1% top-5. As batch size reduces to 4, BN’s accuracy is decreased by 1.2% while the GN remains the same. On the 64-frame input, GN’s accuracy is increased by 1.7% due to longer clip and temporal length, while BN’s accuracy reduces due to lesser batch size.<br /><br />
Extensions:<br />
The applications of Group Normalization on generative models like GANs, sequential models like RNNs and LSTMs, and reinforcement learning models can be analyzed deeply, given their success in replacing Layer Normalization and Instance Normalization in visual recognition tasks. Additionally, there is scope for learning why BN outperforms GN in a few conditions. GN lacks the regularization ability of BN as BN computation involves stochastic batch sampling, which helps in regularization, and this can be analyzed with a regularizer added to GN to improve the results. <br /><br />
Comments:<br />
Upon further research, I found that the extension mentioned in the previous section about the regularizer added with GN was employed in the Big Transfer (BiT): General Visual Representation Learning paper in 2019. Given the excellent performance of GN, I hope to see future models being tuned to GN, which will give better results, and help move deep learning research forward.</p>]]></content><author><name></name></author><category term="paper-review" /><summary type="html"><![CDATA[Group Normalization Paper Review]]></summary></entry><entry><title type="html">Attention is all you need</title><link href="https://vigneshr2306.github.io/blog/2022/Attention/" rel="alternate" type="text/html" title="Attention is all you need" /><published>2022-10-07T23:13:27+00:00</published><updated>2022-10-07T23:13:27+00:00</updated><id>https://vigneshr2306.github.io/blog/2022/Attention</id><content type="html" xml:base="https://vigneshr2306.github.io/blog/2022/Attention/"><![CDATA[<p>Citation: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS’17).<br /><br />
Brief Summary:<br />
Recurrent Neural Networks (RNN), Long Short Term Memory (LSTM) and Sequence Sequence Models are popular approaches to solving tasks such as Image Captioning, Langage Translation, etc. However, they deal with the input in a sequential manner, which is often time-consuming and does not use the parallel processing power of modern computing techniques. Sequential data processing also has the disadvantages of the popular vanishing gradient problem when the sequence is too long. This paper solves these issues by proposing a novel architecture called the Transformer, which is solely based on attention mechanisms. The architecture uses an encoder-decoder model with I/O embedding and positional encoding layers. The Multi-Headed Attention and Masked Multi-Headed Attention layers allow dependency modeling without considering their distance in the input or output sequences. The model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task.<br /><br /></p>

<p>Main Contributions:<br />
The recurrent layers most frequently employed in encoder-decoder designs are replaced with multi-headed self-attention in this study, which is the first transduction model based purely on attention.
The paper achieves state-of-the-art results in language translation tasks and paved the way for future research in this field for papers such as BERT and ViT.<br /><br />
Strengths:<br />
The transformer can be trained significantly faster than architectures based on recurrent or convolutional layers for translation tasks.
In situations when the sequence length is less than the representation dimensionality, self-attention layers perform faster than recurrent layers. <br />
The paper is well-written and easy to read. The code is provided which helps in reproducing the results for further research.<br /><br />
Weakness:<br />
The paper lacks mathematical explanation in parts such as multi-headed attention.<br />
The model contains too many hyperparameters, without tests for significant configurations.<br />
The paper also lacks figures and graphs that could help analyze the results better.<br /><br /></p>

<p>In-Depth Analysis of Strengths and Weaknesses:<br />
RNNs and LSTMs are usually employed for tasks such as image captioning, which consists of recurrent layers. This is replaced with multi-headed self-attention, which is the first transduction model based purely on attention. This block is used to give the word embedding inputs more contextual information. A self-attention module functions by reweighing the word embeddings of each word to account for contextual importance and comparing each word in the phrase to every other word in the sentence, including itself. The algorithm receives N word embeddings without context and outputs N word embeddings with context. <br />
The model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, which beats state-of-the-art results. The paper is also well written, with code provided for reproducibility, which is the reason that led to revolutionizing the industry with several transformer-like architectures.<br />
The transformers can take in parallel inputs, which helps in using the modern GPU parallel processing power. This makes transformers train several times faster than LSTMs. Even though LSTMs are used to avoid vanishing gradient problems, when the sequential input is too long, there exists a vanishing gradient, and this can be avoided using transformers as they process the inputs in parallel.<br />
As for the weakness of the paper is concerned, the paper consists of several hyperparameters, but the significance of the configurations could have been explained better. The authors could have also added some more mathematical explanations on the multi-headed attention part.<br /><br />
Experimental Results:<br />
The Adam optimizer was used to train the design, and the learning rate was first raised and then reduced. In order to avoid overfitting, residual dropout was employed as a regularization strategy. Label smoothing was also utilized so that the model gradually learns to be more uncertain, improving accuracy as a result of more data-driven learning.<br />
The transformer model performs better than the previous state-of-the-art by over 2 BLEU in the English-to-German translation, with a score of 28.4. The training was done for 3.5 days on 8 P100 GPUs. For the English-to-French translation, the BLEU score turned out to be 41.0. <br />
Different model variations are done on the model to evaluate different parts of the transformer. It is observed that reducing the attention key size reduces model quality. It is also seen that bigger models are better, and dropout is very helpful in avoiding over-fitting.<br /><br />
Extensions:<br />
The transformer model that is explained in this paper has been tested only with machine translation models. This can be extended to applications such as image, audio, and video processing. <br /><br />
Comments:<br />
In fact, the state-of-the-art in object detection and image segmentation in the industry uses transformers in it. This would not have been possible without this paper, and hence shows how much this paper has impacted the development of this field.</p>]]></content><author><name></name></author><category term="paper-review" /><summary type="html"><![CDATA[Attention is all you need Paper Review]]></summary></entry><entry><title type="html">AlexNet</title><link href="https://vigneshr2306.github.io/blog/2022/Alexnet/" rel="alternate" type="text/html" title="AlexNet" /><published>2022-09-30T23:13:27+00:00</published><updated>2022-09-30T23:13:27+00:00</updated><id>https://vigneshr2306.github.io/blog/2022/Alexnet</id><content type="html" xml:base="https://vigneshr2306.github.io/blog/2022/Alexnet/"><![CDATA[<p>Citation: Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey E. (2017-05-24). “ImageNet classification with deep convolutional neural networks” Communications of the ACM. 60 (6): 84–90. doi:10.1145/3065386. ISSN 0001-0782. S2CID 195908774, 2012 <br /><br />
Brief Summary:<br />
AlexNet is an 8-layer neural network that can recognize images based on visual information from large image datasets. Consisting of 5 convolutional layers and 3 fully connected layers, AlexNet was able to achieve ground-breaking results in the ImageNet LSVRC-2010 challenge, reaching a top-1 error rate of 37.5%, which was superior to earlier state-of-the-art techniques’ 47.1%. Utilizing the ‘dropout’ to reduce overfitting and using the ReLU non-linearity as the activation function, along with leveraging several GPUs for training were a total game changer that led to these exceptional results.<br /><br /></p>

<p>Main Contributions:<br />
The usage of the ReLU non-linearity over the counterparts such as tanh/sigmoid activation functions can result in faster results.
Overfitting in a large dataset can be avoided using ‘dropout’.
The training error of a model can be reduced by using weight decay.
Publicly available resources of the highly optimized GPU implementation of 2D convolution.<br /><br />
Strengths:<br />
Unleashed the true power of purely supervised learning.
The local normalization scheme helps in generalization.
Overlapping pooling layers are less prone to overfit.
A smart way of augmenting data, keeping in mind the computational constraints.<br /><br />
Weakness:<br />
The paper lacks mathematical rigor, and explains in a more theoretical aspect.
Even though GPU implementation was highly optimized, they are not relevant to current computational power.
Results are focused more on experiments with different layers, but does not focus on optimization. <br /><br />
In-Depth Analysis of Strengths and Weaknesses:<br />
The popularity of ReLU skyrocketed after this paper due to its faster results. This is due to the fact that the usage of ReLU does not lead to saturation of neurons in the neural network, which happens in tanh or sigmoid where the derivative might go close to zero. <br /><br />
Two major reasons for AlexNet to successfully avoid overfitting are the usage of data augmentation and dropout. By cropping random patches of 227x227 from the input 256x256 images and by horizontal reflections, the authors were able to significantly decrease overfitting. To make the images illumination invariant, the authors used the Principal Component Analysis on the RGB values to reduce the top-1 error percentage by 1%. Dropout restricted the number of neurons that participated in backpropagation by setting 50% of the output of hidden neurons to zero. This is implemented by multiplying the outputs by 0.5 at test time. This method significantly forced the neurons to learn more complex features that were useful.<br /><br />
The model’s training error is significantly reduced by the weight decay and it is one of the important factors to consider, since small amount of weight decay resulted in huge reduction in training error. The highly optimized GPU implementation involved two GPUs having 3GB memory each that helped in reducing the top-1 and top-5 error rates by 1.7% and 1.2% respectively.<br /><br />
Another advantage of employing ReLU activation was that the inputs did not require to be normalized, and the local normalization scheme helps in generalization. This involves normalizing the pixel coordinates with the same coordinates from adjacent pixels. This reduced their error by 1.4% and 1.2% respectively. Overlapping pooling was also a great idea to prevent overfitting, where they used stride 2 with filter 3x3 instead of the traditional method of having stride size being the same as filter width. This helped in reducing their error by 0.4% and 0.3%, respectively.<br /><br />
Coming to the weakness, the paper heavily assumes theoretical knowledge of past literature and does not explain mathematically (with equations) on what approach is being taken. I feel that they did their best with the computational power they had in 2012, and Moore’s law worked in doubling the transistors and hence the computational power over the years, which makes it think it is slightly irrelevant for their optimizations now in 2022. Similarly, the paper had deep layers in its time, but comparing it with further models like VGGNet and ResNet, it feels nowhere close. AlexNet definitely paved the way for all these models.<br /><br />
Experimental Results:<br />
The paper involved experimenting with different layers, and having 1 CNN layer resulted in an error rate of 18.2%, and having 5 error rates resulted in 16.4%. Compared to previous state-of-the-art methods such as Sparse Coding, SIFT and Fisher Vectors, error rates are significantly less in AlexNet by over 10%. The best version of their results were the ones where they used additional 2 CNN’s along with the 5 CNN’s to produce an error rate of 15.3%.<br /><br /></p>

<p>Extensions:<br />
In order to simplify the experiments, the authors did not use unsupervised pretraining, which would have helped them to get a good prior to start the network near a global minimum instead of a local minimum, which helps in drastically reducing the training error. This would also help the model to yield a better generalization, which would have resulted in a better regularization. Additionally, the usage of global average pooling can result in a better relationship between feature maps and categories, when compared to Fully Connected Layers.<br /><br />
Comments:<br />
Overall, the paper was definitely way ahead of its time with respect to introducing CNNs for image classification. However, this paper focusses only on image classification, and is dataset specific, instead of a more generalized CNN application which can be used for different applications such as Natural Language Processing, Speech recognition, etc.</p>]]></content><author><name></name></author><category term="paper-review" /><summary type="html"><![CDATA[AlexNet Paper Review]]></summary></entry></feed>